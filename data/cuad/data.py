import pandas as pd
from typing import List, Tuple, Dict, Optional
import warnings
import sys
import re
import json
import os
import numpy as np
from datasets import load_from_disk


from data.dataset import SpecificationCollection, Specification, FixedSpecification
from data.actions import get_classification_actions, get_annotator_output_action
from utils.misc import (
    subset_data,
    parse_json,
    add_section,
    parse_list,
    fuzzy_match,
    parse_for_answer_tags,
)
from data.reward import get_avg_score
from utils.streamlit_types import FormElement, DisplayElement

DATASET_ROOT = os.path.dirname(os.path.abspath(__file__))
DEV_FRAC = 0.1
TRAIN_DOCUMENTS_FRAC = 0.1  # Use 10% of data for prototypes
MAX_TEST_DOCUMENTS = 500
MAX_TRAIN_DOCUMENTS = 5


_remove_whitespace = lambda x: re.sub(r"\s+", " ", x).strip()
_remove_excess_newlines = lambda x: re.sub(r"(\s)*\n(\s)*\n+", "\n", x).strip()
_remove_empty_md_headings = lambda x: re.sub(
    r"^#+\s*[\u200B\u200C\u200D\uFEFF]*$", "", x, flags=re.MULTILINE
)

FIXED_INSTRUCTIONS = """
Span extraction is the task of extracting spans from a text snippet. For example, an annotator might be trained to extract spans from a text snippet that match the legal concept "Licensor IP." The key to good span extraction is writing **clear annotation instructions explaining what each label means and what to do in potential edge cases.**

However, writing those annotation instructions is difficult. It takes a lot of trial and error to capture the pattern of how the labels are assigned.

### What you need to prompt the assistant to do
In this task, **your goal is to get the assistant to write a document of annotation instructions for a specific span extraction task.** When the chat session starts, you will see information about the span extraction task on the left side of the screen.

You can put anything that would be helpful in the instructions: nothing is off limits.
## Maximizing your score
**We provide a tool that allows you to measure the quality of your annotation instructions.** 
Scores range from 0\% to 100\%. This score forecasts the accuracy of later annotators, should you give them those instructions. 

To maximize your score, you may have to try different instructions and compare their scores.
"""


def render_fixed_task_explanation():
    """Render the fixed task explanation for CUAD."""
    st.markdown(FIXED_INSTRUCTIONS)


def keep_only_top_level_bullets(x: str) -> str:
    """
    remove any lines that are indented bullets

    DO NOT REMOVE:
    *   Any definition of the Licensor IP that includes the IP owned by the licensor’s affiliates should be labeled together with the License Grant clause.

    DO REMOVE:
        *   Example to label
            *   Distributor hereby grants Zogenix an irrevocable, perpetual license with the right to grant sublicenses to use such Data. <omitted> “Data” means any and all data pertaining to the Product in the Field that is generated by or under the authority of Distributor or its Affiliates, Sub-distributors or other subcontractors.
    """
    s = "\n".join([re.sub(r"^(\s)+\*(.)+$", "", xi).strip() for xi in x.split("\n")])
    return _remove_excess_newlines(s)


class CUADDataset(SpecificationCollection):
    """
    Contract Understanding Atticus Dataset (CUAD) - A dataset for contract understanding and review.

    Paper: https://arxiv.org/abs/2103.06268
    Dataset: https://huggingface.co/datasets/theatticusproject/cuad-qa
    """

    @property
    def dataset_name(self) -> str:
        return "cuad"

    @property
    def dataset_pretty_name(self) -> str:
        return "Text Annotation"

    @property
    def dataset_description(self) -> str:
        return "Work with the assistant to **write annotation guidelines for a text annotation problem**. The documents are specifically legal contracts."

    @property
    def assets_file_id(self) -> str:
        return "16Xem9oNc4-lnjnC_W_24-KwlbwggJ_a1"

    def _create_user_expertise_form(self) -> List[FormElement]:
        """Create the user expertise form for CUAD."""
        return [
            FormElement(
                input_type="radio",
                label="How familiar are you with concepts from US contract law?",
                options=["Beginner", "Intermediate", "Advanced", "Expert"],
                default="Intermediate",
                required=True,
                help="This helps us understand your legal expertise level",
            )
        ]

    def __init__(
        self,
        dev: bool = False,
        allow_partial: bool = True,
        models: List[str] = ["gpt-5-mini"],
        fixed_indexes: Optional[List[int]] = None,
        allow_multimodal_actions: bool = False,
        **kwargs,
    ) -> None:
        super().__init__(dev=dev, **kwargs)

        # Load all the data
        split = "train" if dev else "validation"
        self._ds = load_from_disk(f"{DATASET_ROOT}/assets/cuad-{split}")
        # Group by category
        categories = list(sorted(set(self._ds["codebook_filename"])))
        self._categories = subset_data(categories, DEV_FRAC, 1.0, dev)

        # Calculate total length across all question types
        self.fixed_length = len(self._categories)
        self.custom_length = 0  # No custom specifications for CUAD
        self._allow_partial = allow_partial
        self._models = models
        self._allow_multimodal_actions = allow_multimodal_actions

        # All subclasses must have these attributes set
        self._finish_init()

        if fixed_indexes is not None:
            self._load_fixed_specs(fixed_indexes)

    def _load_fixed_specs(
        self, indexes: Optional[List[int]] = None
    ) -> Dict[int, FixedSpecification]:
        if indexes is None:
            return {}

        # Load requested specs
        specs = {}
        for ix in indexes:
            category = self._categories[ix]
            items = self._ds.filter(lambda x: x["codebook_filename"] == category)
            name = category.replace(".md", "").replace("-", " ")
            # Combine any rows that have the same context (same contract)
            context_to_items = {}
            for item in items:
                context = item["context"]
                if context not in context_to_items:
                    context_to_items[context] = []
                context_to_items[context].append(item)
            combined_items = []
            for context, items in context_to_items.items():
                all_answers = [
                    answer for item in items for answer in item["answers"]["text"]
                ]
                combined_items.append(
                    {
                        "context": context,
                        "question": items[0]["question"],
                        "answers": {"text": all_answers},
                        "codebook_filename": items[0]["codebook_filename"],
                        "parent_page": items[0]["parent_page"],
                    }
                )
            items = combined_items

            # Get the handbook entry and parent page
            handbook_file = os.path.join(
                DATASET_ROOT, "assets/handbook", items[0]["codebook_filename"]
            )
            if not os.path.exists(handbook_file):
                continue

            # Read the handbook entry
            with open(handbook_file, "r") as f:
                handbook_text = f.read()
            handbook_text = _remove_excess_newlines(
                _remove_empty_md_headings(handbook_text)
            )

            # Create dataset
            data = {
                _remove_excess_newlines(item["context"]).strip().replace("\n", " "): [
                    y.strip().replace("\n", " ") for y in item["answers"]["text"]
                ]
                for item in items
            }
            train_data = subset_data(
                data, TRAIN_DOCUMENTS_FRAC, 1.0, True, max_len=MAX_TRAIN_DOCUMENTS
            )
            test_data = subset_data(
                data, TRAIN_DOCUMENTS_FRAC, 1.0, False, max_len=MAX_TEST_DOCUMENTS
            )  # False to get second portion

            # create signature / codebook
            signature = f"In this text annotation task, the input is a legal contract. The annotator needs to extract all spans in the contract that match the legal concept {name.title()}."
            signature_multimodal = [
                DisplayElement(
                    input_type="markdown",
                    value=signature,
                ),
                DisplayElement(
                    input_type="dataframe",
                    value=pd.DataFrame(
                        list(train_data.items()),
                        columns=["input", "correct_annotations"],
                    ),
                ),
            ]
            signature += "\n\n" + add_section(
                "Labeled examples", fmt_examples(train_data)
            )

            theta = add_section(
                f"Explanation of the concept {name.title()}", handbook_text
            )
            theta = theta.replace("\n*   ", "\n<chunk>\n*   ")

            fmt_instructions = "Return a markdown document of annotation guidelines for this task. Wrap the document in <instructions></instructions> tags."
            ystar = signature + "\n\n" + theta

            # Create the spec
            spec = FixedSpecification(
                dataset_name=self.dataset_name,
                index=f"fixed_{ix}",
                full_specification=theta,
                initial_specification=signature,
                initial_specification_multimodal=signature_multimodal,
                validity_fn=validity_fn,
                reward_fn=reward_fn,
                reward_kwargs={
                    "allow_partial": self._allow_partial,
                    "test_dataset": test_data,
                    "models": self._models,
                },
                # reward_fn_tool_name=None,  # Not provided
                # reward_fn_tool_description=None,  # Not provided
                ystar=ystar,
                metric_name="F1",
                # baseline_scores=None,  # Not provided
                render_task_explanation=render_fixed_task_explanation,
                actions=get_classification_actions(
                    train_data,
                    test_data,
                    fuzzy_match_x=lambda x1, x2: fuzzy_match(
                        x1,
                        x2,
                        ignore_whitespace=True,
                        ignore_punctuation=True,
                        ignore_case=True,
                        allow_substring=True,
                    ),
                    fuzzy_match_x_error_msg="Could not recognize the input document.  Make sure the input exactly matches the full document of interest.",
                    fuzzy_match_y=lambda y1, y2: _normalize_into_list(y1)
                    == _normalize_into_list(y2),
                    return_as_df=self._allow_multimodal_actions,
                )
                + get_annotator_output_action(self._models),
                fmt_instructions=fmt_instructions,
                name=name,
                user_expertise_form=self._create_user_expertise_form(),
                # test_dataset=test_data,  # Not in base class
                # metric_name="F1",  # Not in base class
            )
            specs[ix] = spec
        return specs


def fmt_examples(examples: Dict[str, List[str]]) -> str:
    return "\n\n".join(
        [
            add_section(
                f"Example {i + 1} contract text",
                input,
                style="code",
            )
            + "\n"
            + add_section(
                f"Example {i + 1} correct spans",
                "[" + ", ".join([f"'{o}'" for o in output]) + "]",
                style="code",
            )
            for i, (input, output) in enumerate(examples.items())
        ]
    )


def validity_fn(yhat: str, raise_errors: bool = False) -> bool:
    valid = "<instructions>" in yhat and "</instructions>" in yhat
    if not valid and raise_errors:
        raise Exception(
            "Could not parse annotation guidelines from the output. Make sure the output is a valid markdown document of annotation guidelines. Wrap the document in <instructions></instructions> tags."
        )
    return valid, {}


def reward_fn(
    yhat: str,
    test_dataset: Dict[str, str],
    models: List[str],
    label_set: List[str],
    allow_partial: bool = False,
    raise_errors: bool = False,
) -> Tuple[float, dict]:
    parsed_yhat = parse_for_answer_tags(
        yhat, keyword="instructions", return_none_if_not_found=True
    )
    if parsed_yhat is None:
        parsed_yhat = yhat

    if allow_partial:
        score_fn = score_partial
    else:
        score_fn = score_exact

    scores, outputs = get_avg_score(yhat, test_dataset, models, score_fn)
    return (
        np.mean([v for v in scores.values() if v is not None]) * 100,  # scale to 0-100
        {"outputs": outputs},
    )


def _normalize_into_list(y: str) -> List[str]:
    if not isinstance(y, str):
        return y
    try:
        y = eval(y)
    except:
        y = [y]
    return y


def score_partial(z: List[str], zhat: str, raise_errors: bool = False) -> float:
    z = [_remove_whitespace(zi) for zi in z]
    try:
        zhat = _normalize_into_list(zhat)
    except:
        zhat = None
    if zhat is None:
        return 0

    zhat = [_remove_whitespace(zhati) for zhati in zhat]

    if len(zhat) == 0:
        return 1 if len(z) == 0 else 0
    if len(z) == 0:
        return 1 if len(zhat) == 0 else 0

    recall_array = [any(zi in zhati for zhati in zhat) for zi in z]
    precision_array = [any(zi in zhati for zi in z) for zhati in zhat]

    recall = sum(recall_array) / len(recall_array)
    precision = sum(precision_array) / len(precision_array)
    return (
        (2 * (precision * recall) / (precision + recall))
        if (precision + recall) > 0
        else 0
    )


def score_exact(z: List[str], zhat: str, raise_errors: bool = False) -> float:
    z = [_remove_whitespace(zi) for zi in z]
    try:
        zhat = parse_json(zhat)
    except:
        zhat = None
    if zhat is None:
        return 0

    zhat = [_remove_whitespace(zhati) for zhati in zhat]

    recall_array = [zi in zhat for zi in z]
    precision_array = [zhati in z for zhati in zhat]

    if len(recall_array) == 0:  # GT is empty
        return 1 if len(zhat) == 0 else 0

    recall = sum(recall_array) / len(recall_array)
    precision = sum(precision_array) / len(precision_array) if precision_array else 0
    return (
        (2 * (precision * recall) / (precision + recall))
        if (precision + recall) > 0
        else 0
    )
